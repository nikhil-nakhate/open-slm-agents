extends: base

model:
  params:
    max_seq_len: 1024
    dropout: 0.1
    vocab_size: 50257  # Standard GPT-2 vocab size
  
  modules:
    tokenizer:
      kind: tiktoken
      params:
        name: gpt2
        add_prefix_space: true
      freeze: false
    
    transformer:
      dim: 768
      n_layers: 12
      n_heads: 12

train:
  # Optimizer settings - from notebook
  optimizer:
    lr: 0.00005  # Notebook uses 0.00005
    betas: [0.9, 0.95]
    weight_decay: 0.1
  
  # Training loop settings - from notebook
  batch_size: 8  # Notebook uses 8
  max_steps: 2000  # Increased for better training
  save_every: 200
  log_every: 20
  eval_every: 5  # Notebook evaluates every 5 steps
  max_eval_batches: 5  # Notebook uses 5 batches for eval
  amp: true
  
  # Training stability
  grad_clip: 1.0
  grad_accum_steps: 1
  
  # Data settings
  data_dir: data
  data_loader:
    kind: sft_json
    block_size: 1024
    shuffle: true
    num_workers: 2
    pin_memory: true
    drop_last: false
  
  # Data splits
  splits:
    ratios:
      train: 0.85
      eval: 0.10
      test: 0.05
    seed: 42
  
  # Output settings
  output_dir: outputs/gpt2-base
  run_name: gpt2-base
  
  # Logging settings
  project: open-slm
  log_dir: runs
  
  # Learning rate scheduler (set to null to disable)
  scheduler:
    kind: null
    warmup_steps: 100
    min_lr: 0.0

data:
  transforms:
    template: alpaca

eval:
  # Generation parameters
  max_new_tokens: 80
  temperature: 0.9
  top_k: 40
  top_p: 0.0
  greedy: false
