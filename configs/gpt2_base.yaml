extends: base

model:
  name: gpt
  params:
    max_seq_len: 1024
    dropout: 0.1
    vocab_size: 50257  # Standard GPT-2 vocab; if tokenizer changes, this can be removed
  modules:
    tokenizer:
      kind: hf_gpt2
      params:
        name: gpt2
        add_prefix_space: true
      freeze: false
    token_embedding:
      freeze: false
    position_embedding:
      freeze: false
    emb_dropout:
      p: 0.1
    transformer:
      dropout: 0.1
      mlp_mult: 4
      activation: gelu
      dim: 768
      qkv_bias: false
      n_layers: 12
      n_heads: 12
      prenorm: true
      freeze: false
    output_projection:
      tie_weights: true
      freeze: false
    loss:
      kind: cross_entropy
      params:
        ignore_index: -100
      freeze: false

train:
  lr: 0.0002
  betas: [0.9, 0.95]
  weight_decay: 0.1
  batch_size: 4
  max_steps: 1000
  save_every: 200
  log_every: 20
  amp: true
  data_dir: data
  output_dir: outputs/gpt2-base
  project: open-slm
  log_dir: runs
  run_name: gpt2-base
  scheduler:
    kind: cosine
    warmup_steps: 100
    min_lr: 0.0
  data_loader:
    kind: language_modeling_text
    block_size: 1024
    shuffle: true
    num_workers: 2
    pin_memory: true
    drop_last: false
  splits:
    ratios:
      train: 0.85
      eval: 0.10
      test: 0.05
    seed: 42

data:
  transforms:
    template: alpaca

eval:
  max_new_tokens: 80
  temperature: 0.9
  top_k: 40
  top_p: 0.0
  greedy: false
