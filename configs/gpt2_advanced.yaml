extends: gpt2_base

train:
  # Optimizer settings - more conservative
  optimizer:
    lr: 0.0001
    betas: [0.9, 0.999]
    weight_decay: 0.01
    eps: 1e-8
  
  # Training loop settings
  batch_size: 2
  max_steps: 5000
  save_every: 500
  log_every: 50
  eval_every: 200
  max_eval_batches: 200
  amp: true
  
  # Training stability - more aggressive
  grad_clip: 0.5
  grad_accum_steps: 4
  
  # Learning rate scheduler - longer warmup
  scheduler:
    kind: cosine
    warmup_steps: 500
    min_lr: 0.0
  
  # Output settings
  output_dir: outputs/gpt2-advanced
  run_name: gpt2-advanced

model:
  modules:
    transformer:
      # Larger model
      dim: 1024
      n_layers: 24
      n_heads: 16
      dropout: 0.1
      qkv_bias: true
