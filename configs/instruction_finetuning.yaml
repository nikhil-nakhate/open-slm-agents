extends: gpt2_medium

train:
  # Optimizer settings - from notebook
  optimizer:
    lr: 0.00005  # Notebook's optimal learning rate
    betas: [0.9, 0.95]
    weight_decay: 0.1
  
  # Training loop settings - from notebook
  batch_size: 8
  max_steps: 1000  # 2 epochs * ~500 steps per epoch
  save_every: 100
  log_every: 5
  eval_every: 5  # Notebook evaluates every 5 steps
  max_eval_batches: 5  # Notebook uses 5 batches for eval
  amp: true
  
  # Training stability
  grad_clip: 1.0
  grad_accum_steps: 1
  
  # Data settings - instruction fine-tuning
  data_dir: data/datasets/sft  # Point to correct path
  data_loader:
    kind: sft_json  # Use instruction fine-tuning dataset
    block_size: 1024
    shuffle: true
    num_workers: 0
    pin_memory: true
    drop_last: true  # Notebook uses drop_last=True for training
  
  # Data splits - from notebook (85% train, 10% test, 5% val)
  splits:
    ratios:
      train: 0.85
      eval: 0.05  # 5% validation
      test: 0.10  # 10% test
    seed: 123  # Notebook uses seed 123
  
  # Output settings
  output_dir: outputs/instruction-finetuning
  run_name: instruction-finetuning

# Model configuration inherited from gpt2_medium

data:
  transforms:
    template: alpaca  # Use Alpaca format for instruction fine-tuning
